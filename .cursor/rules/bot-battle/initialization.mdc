Project name: BotBattle

Project Goal:
Build a frontend-only Next.js + TypeScript app to benchmark and analyze responses from multiple free LLM APIs. Users select predefined prompt templates or custom prompts and receive detailed analytics on LLM performance.

‚∏ª

üåü Integrated LLM Models

Start with these models (all currently have publicly available APIs):
	‚Ä¢	Anthropic Claude (Claude 3 Haiku/Sonnet)
	‚Ä¢	Google Gemini (Gemini 1.5 Pro)
	‚Ä¢	Groq (LLaMA, Mixtral)
	‚Ä¢	DeepSeek
	‚Ä¢	Mistral AI (7B, Mixtral)
	‚Ä¢	Perplexity AI (Sonar series)
	‚Ä¢	Cohere (Command R)
	‚Ä¢	OpenRouter models (various)

‚∏ª

üìå Prompt Templates

Provide clear, reusable prompt templates as selectable options:

General QA
Prompt: "What is the capital of France and why is it historically significant?"

Creative Writing
Prompt: "Write a short story about a lost astronaut discovering an ancient civilization."

Summarization
Prompt: "Summarize the following article into three concise sentences: {user pastes article}"

Code Generation
Prompt: "Generate a Python function that calculates Fibonacci numbers using memoization."

Math Problem Solving
Prompt: "Solve step-by-step: If x¬≤ + 2x - 3 = 0, find the values of x."

Translation
Prompt: "Translate the following English sentence into Spanish: 'Tomorrow will be sunny and warm.'"

Instruction Following
Prompt: "List five effective strategies to improve remote team productivity."

üìê Detailed Metrics for Response Evaluation

üïë Performance Metrics
	‚Ä¢	Latency (Total response time in ms)
	‚Ä¢	First-byte latency (ms)
	‚Ä¢	Tokens per second (TPS)

üìÉ Structural Metrics
	‚Ä¢	Input token count
	‚Ä¢	Output token count
	‚Ä¢	Word count
	‚Ä¢	Character count
	‚Ä¢	Formatting presence (Markdown, Bullet points, Code blocks, Headers)
	‚Ä¢	Length classification (Concise: <50 tokens, Moderate: 50-150 tokens, Verbose: >150 tokens)

üìä Content Quality Metrics (evaluated via additional LLM, such as Claude or GPT-4o)
	‚Ä¢	Accuracy (Factual correctness) ‚Äî Score (1-10)
	‚Ä¢	Relevance (How directly the output addresses the prompt) ‚Äî Score (1-10)
	‚Ä¢	Clarity (Grammar, readability, logical coherence) ‚Äî Score (1-10)
	‚Ä¢	Creativity (Novelty, originality, narrative strength) ‚Äî Score (1-10, primarily for creative prompts)
	‚Ä¢	Readability level (Flesch-Kincaid or GPT evaluated)
	‚Ä¢	Safety/Toxicity evaluation (Safe or flagged)
	‚Ä¢	Bias evaluation (Neutral, mildly biased, strongly biased, evaluated by an LLM)

üîÑ Consistency Metrics
	‚Ä¢	Response determinism (Variation score from multiple identical prompts)

‚∏ª

üéØ Functional Flow for Cursor Agent (Detailed)

Step 1: User Interaction
	‚Ä¢	User chooses from prompt template dropdown.
	‚Ä¢	Alternatively, user can manually type or paste a custom prompt.

Step 2: Model Selection
	‚Ä¢	Users can toggle which LLM APIs to query (checkboxes/multi-select dropdown).

Step 3: Query Execution
	‚Ä¢	Send prompt to selected LLM APIs simultaneously using fetch/Axios.
	‚Ä¢	Record precise timestamps at start and completion of each query.

Step 4: Initial Metrics Computation
	‚Ä¢	Immediately calculate latency, token count, word count, character count.
	‚Ä¢	Visually indicate loading state and partial results to the user.

Step 5: Secondary Metrics Evaluation
	‚Ä¢	Automatically feed each LLM output to an evaluator LLM (e.g., Claude 3 or GPT-4o) to compute advanced metrics (accuracy, clarity, relevance, creativity, toxicity, bias).
	‚Ä¢	Receive evaluation scores, parse, and normalize them for consistent comparison.

Step 6: Results Visualization
	‚Ä¢	Display results clearly side-by-side or in an easy-to-read comparative card view.
	‚Ä¢	Use radar charts or bar graphs to show qualitative metrics clearly.
	‚Ä¢	Allow sorting by latency, accuracy, relevance, creativity, etc.

‚∏ª

üé® Frontend UX/UI Considerations
	‚Ä¢	Clean interface using Tailwind CSS
	‚Ä¢	Dropdown for prompts; clear textual input areas
	‚Ä¢	Interactive model selection interface (checkbox, toggle button)
	‚Ä¢	Visual loading indicators, smooth animations using Framer Motion
	‚Ä¢	Color-coded metrics for immediate visual interpretation
	‚Ä¢	Collapsible response details to keep the main view concise

    üìÅ Suggested Project Structure

    /src
  /components
    - PromptDropdown.tsx
    - PromptInput.tsx
    - ModelSelector.tsx
    - ResponseCard.tsx
    - MetricsChart.tsx
    - LoadingIndicator.tsx
  /hooks
    - useLLMQuery.ts
    - useMetricsEvaluation.ts
  /utils
    - api.ts (handles API calls)
    - evaluators.ts (handles secondary LLM evaluations)
    - metrics.ts (calculations of tokens, words, latency, etc.)
  /data
    - prompts.ts (prompt templates and categories)
    - models.ts (LLM model configs and API details)
  /types
    - index.ts (TypeScript interfaces & enums for models, prompts, metrics)

    üìå Next Steps for MVP
	‚Ä¢	Immediate: Set up Next.js, Tailwind, and basic frontend scaffolding.
	‚Ä¢	Phase 1: Implement prompt selection, custom input, and basic API querying (latency & token counting).
	‚Ä¢	Phase 2: Integrate additional LLM-based analytics (accuracy, creativity, toxicity).
	‚Ä¢	Phase 3: Expand UX, improve visualizations, and add sorting/filtering functionality.

‚∏ª
